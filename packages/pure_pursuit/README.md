# Short-term Slam and Duckiebot Detection

## Project Description

This repository contains the code of our course project, in which we aim to solve two distinct problems related to the LF and LFV challenges.

The first problem is the fact that some measurements, such as the position of white or yellow line segments in the lane, are often received in short bursts, rather than at regular and consistent rates. This makes it difficult to produce robust controllers based on these inputs.

The second is obstacle detection, in particular the detection of other duckiebots. To this end, we leveraged a distinct feature of the duckiebots' backpanel, namely their characteristic grid of black circles on a white background.

We will provide a detailed overview of our approach for each problem here. 

<!-- This experimental demo contains code aims to solve the problem that the lane filter node is not giving enough data to build a robust controller. We will often not have enough measurment of the white or yellow line and it will be difficult to input a good command. -->

## "Short-Term Slam"

The main input to our Pure Pursuit controller is the ground projected relative position of white and yellow line segments from the input image. The white and yellow points of these segments are then used by the controller to produce an estimate of a trajectory the robot should follow.

As previously mentioned, here are some characteristics of the filtered yellow and white line segments that are produced by the lane filter:

1. They are not always available
2. When they become available, they often come in big bursts
3. When they are available, they are accurate (or assumed to be)


Using these characteristics, we implement a buffer that can keep track of the location of previously seen objects (in this demo we use it with the yellow and white lines) and update it at each timestamp using the linear and angular velocities that were sent to the robot. In our case, the pure pursuit controller itself sends this command to the robot by publishing to the the `car_cmd` topic at a programmable frequency of 1/`delta_t`, 10Hz by default.

This buffering mechanism, implemented in [the `PointTracker` class](include/point_tracking/point_tracker.py) is described here.

First, When new measurements (i.e. line points) are received, they are passed to the `add_points` method, which stores their relative position and the time at which they were received in the buffer.
Then, whenever a new car command is issued, it is also passed to the `update_points` method of the `PointTracker`, which performs the following steps: 

1. Discard measurements older than a programmable threshold (`memory_secs`, 1 second by default)

2. Discard measurements that are now further from the robot's current position than a given threshold distance (`max_distance`, 1m by default).

3. Use the provided linear (`v`) and angular (`omega`) velocities of the `car_cmd` message, along with the amount of time elapsed since the last update, `dt`, to update the location of the measurements older than `delta_t` using dead-reckoning.

4. Produce a set of K "observations" (i.e. centroids) by performing a K-Means clustering over the points in the buffer (with K = `num_points_to_observe`)

The above-mentioned parameters and their values can be found within the `.yml` configuration files in the "`config/pure_pursuit_controller_node_better`" folder.

The demo can be launched with the launch file "pure_pursuit.launch". It is possible to switch between the normal version and the augmented line detection version by uncommenting the line:

    <!-- <arg name="node_script" value="pure_pursuit_controller_node"/> -->

# Visualization

To debug the behavior of the algorithm, it is relevant to visualize the new points generated by the short-term Slam. To do so, we re-used the package "duckiebot_visualizer" and added code to publish markers for the augmented segment lists. We also added code to display the follow point for the pure_pursuit algorithm, which helped us in fine tune the controller. A package that contains the modified duckibot_visualizer (name duckiebot_visualizer_local) is included in this repo, and the launch file points to it (it also disables the visualizer from the master launch).

# Vehicle detection

To achieve the LFV challenge, vehicles need to be able to detect other vehicles on the track. This is handled using the vehicle_detection_node, which subscribes to the compressed image topic and publishes information about the detection. This node specifically look for a circle grid, which is displayed on the back panel of a duckiebot. At the moment that this project was done, the simulator did not include a rear-panel on the duckiebot model, so we added one to the .obj file (which was pushed to the official gym-dt repo). Moreover, the simulator did have a calibration for the fisheye, so we were not able to get good result on the circlegrid detection. To mitigate the fisheye effect, we reduced the size of the grid that we were trying to detect. Hence, the software is set to detect a line of 3x1 circle (since it is the only size that we were able to detect with the fisheye effect), though it should not be a problem anymore since the simulator now provides proper undistorting matrices. Moreover, this should not be a problem on the real robot. Hence, it is recommended to tune the grid detection parameters to the size of the grid (or maximum size that can be detected correctly).

If a duckiebot is detected, we set the finite state machine (FSM node) to a stopping state. For our purpose, we reused an existing state that was mapped to a stop command: `ARRIVE_AT_STOP_LINE` and we set the state directly using the setFSM service offered by the FSM node. We included a local copy of the FSM node since the proper way of changing state would be to add new transitions, but it is not yet implemented (but the stopping works properly using the service).


# Result

## Simulation:

The short-term slam helps the pure-pursuit better handle turns when the robot is facing the open field (before turning) and is not seeing any points. However, the buffer sometime generates outlier points which can confuse the pure_pursuit. Since the refresh rate is hight enough, even if we have a false follow point, the end result is not bad. The point buffer is definitely an improvement from the calssical lane_filter, though if he had more time we could tune it to get rid of outlier and improve the overall robustness. One idea would be to add weigths on points, based on how old they are or on our confidence level.

## Real robot:

The vehicle detection works properly, but the overall controller seems to be having problem on turns. It acts has if it was not detecting any lines for some specific right turns.

